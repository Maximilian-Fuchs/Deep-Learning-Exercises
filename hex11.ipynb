{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL4NLP SS17 Home Exercise 11\n",
    "----------------------------------\n",
    "**Due until Friday, 21.07. at 13:00**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 Data Inspection (2P)\n",
    "As an accompanying data set to this home exercise, you'll find two POS tagged corpora: one based on the English Brown corpus and the other based on the German Tiger corpus.\n",
    "\n",
    "### Task 1.1 Corpora (1P)\n",
    "Take a look at the corpora. How many different POS labels are there in both corpora? How many sentences are there in `de_100` and in `de_10000`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 Word Embeddings (1P)\n",
    "You can also find bilingual word embeddings for German and English, named ``de_embeddings.txt`` and ``en_embeddings.txt``, respectively. Verify that these are bilingual embeddings: What are the 5 closest German words for English \"wolf\"? How about English words for the German word \"katze\"?\n",
    "\n",
    "**Hint:** gensim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 Multi-Task Learning (8P)\n",
    "This task revolves around applying a given multi-task learning tool (implemented by Tobias Kahse using Tensorflow) on the data from task 1.\n",
    "\n",
    "### Task 2.1 MTL Framework installation & Familiarization (1P)\n",
    "Have a look at the MTL framework available in the folder ``mtl-sequence-tagging-framework``. In particular, look at the `README.md`. Install the framework on your local machine. Note that this framework uses python2.\n",
    "\n",
    "According to the `README.md`, which are the commands to train a model and to evaluate a trained model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 Configuration file (2P)\n",
    "Take a look at the accompanying configuration file ``example.yaml``.\n",
    "* How many and which tasks are specified in the configuration file?\n",
    "* Which activation type is used for the two tasks?\n",
    "* Which is the 'lower level task' and which is the 'higher level task' according to the configuration file?\n",
    "* Which early stopping strategy is used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3 STL Learning (1P)\n",
    "Adapt the configuration file such that it performs single task learning (STL) on the data set ``de_100`` (train and dev). As test set you should use ``de_10000``. What is your accuracy on the German test set?\n",
    "\n",
    "**Hyperparameters**: Use CRF layers, no softmax activations. Be sure to adapt the path to your embeddings; also set the proper embedding size. Set `num_runs` to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3 MTL Learning (1P)\n",
    "Now write a configuration file in which you specify two tasks, German POS tagging as one task and English POS tagging as second task. For the English data, use ``en_1000`` (train+dev). What is your accuracy when you evaluate on ``de_10000`` as before?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4 Comparison (1P)\n",
    "In a sense, comparing the results of task 2.3 to those of task 2.4 can be seen as unfair because more data has been used in the MTL setting compared to the STL setting. Specify a meaningful scenario for comparing STL to MTL (0.5P). Which accuracy do you get in this case? (0.5P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.5 Interpretation (2P)\n",
    "How do you explain your findings from task 2.4? Why do you think is your MTL learning better/worse than STL? In a real usage scenario, which steps would you take to ascertain that your claims are correct?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
