{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "Watch this week's mandatory video and answer the following question: Which six activation functions are discussed during the talk?\n",
    "\n",
    "Sigmoid, tanh, ReLU, Maxout, ELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.1 Dataset reader (1P)\n",
    "Implement a reader for the dataset files which returns the input vectors $\\mathbf{x}$ and labels $y$ as numpy arrays. The shape and number of returned arrays is up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filereader(textfile):\n",
    "    #read file from disk into memory\n",
    "    f = open(textfile)\n",
    "    raw = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    entries = raw.split('\\n')\n",
    "    data = []\n",
    "    for i in range(len(entries)):\n",
    "        if(len(entries[i].split('\\t')) > 2):\n",
    "            review = entries[i].split('\\t')[0]\n",
    "            if(entries[i].split('\\t')[1][-3] == 'N'):\n",
    "                polarity = np.array(0, dtype='float_')\n",
    "            else:\n",
    "                polarity = np.array(1, dtype='float_')\n",
    "\n",
    "            word_vektor = np.array(entries[i].split('\\t')[2].split(' '), dtype='float_')\n",
    "            word_vektor = np.append(word_vektor, 1)\n",
    "            data.append([review, polarity, word_vektor])\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = filereader('DATA/rt-polarity.train.vecs')\n",
    "dev_data = filereader('DATA/rt-polarity.dev.vecs')\n",
    "test_set = filereader('DATA/rt-polarity.test.vecs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.2 Numpy implementation (5P)\n",
    "\n",
    "a) Implement the perceptron stated above only using numpy. Include a method which computes the square loss and the accuracy of the model, given a dataset and a weight vector `w`. (3P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x, w):\n",
    "    return 1/(1+np.exp(-1*np.dot(x, w).astype('float_')))\n",
    "\n",
    "def dsigmoid(x, w):\n",
    "    return sigmoid(x, w)*(1-sigmoid(x, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update(train_data, w, batch_size, rate):\n",
    "    #rate = 0.01\n",
    "    l = len(train_data)\n",
    "    np.random.shuffle(train_data)\n",
    "    #batch_size = np.random.randint(l)\n",
    "    \n",
    "    batches = [train_data[i:i + batch_size] for i in range(0, l, batch_size)]\n",
    "    for mini_batch in batches:\n",
    "        mini_batch = [(entry[2].reshape(1,101), entry[1]) for entry in mini_batch]\n",
    "    \n",
    "        w = w - (rate/batch_size)*sum((sigmoid(x_y[0],w)-x_y[1])*dsigmoid(x_y[0],w)*x_y[0].transpose()\n",
    "                for x_y in mini_batch)\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loss(data, w):\n",
    "    xy_data = [(entry[2], entry[1]) for entry in data]\n",
    "    return int(sum((sigmoid(x_y[0],w)-x_y[1])**2\n",
    "            for x_y in xy_data))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(data, w):\n",
    "    P = sum(1 for entry in data if entry[1]==1)\n",
    "    N = sum(1 for entry in data if entry[1]==0)\n",
    "    TP = sum(1 for entry in data if \n",
    "              sigmoid(entry[2],w) >= 0.5 and entry[1]==1)\n",
    "    TN = sum(1 for entry in data if \n",
    "              sigmoid(entry[2],w) < 0.5 and entry[1]==0)\n",
    "    accuracy = (TP+TN)/(P+N)\n",
    "    \n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Train your perceptron on the training data and observe its accuracy on the development set. Start with batch size |T′|=10, learning rate α=0.01α=0.01 and 100 epochs. Experiment with different values for these three hyperparameters. Can you find a configuration which beats 70% accuracy on the development set? Report your best configuration and both the loss and accuracy it reaches on the development and test sets. (1P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runepoch(rate, batch_size, epochs, dataset):\n",
    "    N = 101\n",
    "    w = np.random.normal(0,1,(N,1))\n",
    "    for i in range(epochs):\n",
    "        w = update(train_data, w, batch_size, rate)\n",
    "    loss = get_loss(dataset, w)\n",
    "    acc = accuracy(dataset, w)\n",
    "    print(epochs,'\\t',batch_size,'\\t',rate,'\\t',str(acc)[:5],'\\t',loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Epochs\\t BSize\\t Rate\\t Acc\\t Loss')\n",
    "runepoch(0.01,10,100, dev_data)\n",
    "runepoch(1,10,100, dev_data)\n",
    "runepoch(0.01,100,100, dev_data)\n",
    "runepoch(0.01,10,1000, dev_data)\n",
    "\n",
    "\n",
    "runepoch(1,10,100, test_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An increase of learning rate showed the best result when evaluating on teh developement set.\n",
    "\n",
    "With a learning rate of 1, 100 epochs and batch size 10 the perceptron reached an accuracy of 0.503 and loss of 793 on the test set."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Epochs\t BSize\t Rate\t Acc\t Loss\n",
    "100 \t 10 \t 0.01 \t 0.674 \t 514\n",
    "100 \t 10 \t 1 \t     0.678 \t 513\n",
    "100 \t 100 \t 0.01 \t 0.508 \t 783\n",
    "1000 \t 10 \t 0.01 \t 0.507 \t 786\n",
    "\n",
    "Test:\n",
    "100 \t 10 \t 1 \t 0.503 \t 793"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Create a plot similar to the one from lecture 02, slide 18, i.e. plot the loss on the training set and the development set vs. the number of training epochs. For this purpose, run your perceptron with batch size |′|=1|T′|=1 and learning rate α=0.001α=0.001 for a large number of epochs (>2500). Which number of epochs is reasonable? Why does the loss on the development set not increase over time, contrary to the figure from the lecture? (1P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1/np.exp(800))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
