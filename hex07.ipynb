{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL4NLP SS17 Home Exercise 07\n",
    "----------------------------------\n",
    "**Due until Tuesday, 06.06. at 13:00**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 Text Classification on 20 Newsgroups (10P)\n",
    "\n",
    "In this exercise, the 20 Newsgroups dataset is being examined. It is a collection of e-mails coming from different newsgroups. The goal is to assign each e-mail its corresponding newsgroup.\n",
    "\n",
    "** Warning: There is not much code to write in this exercise, but training CNNs is a computational intensive task! Plan accordingly for the extended training times. **\n",
    "\n",
    "\n",
    "#### Hints on the Submission Format\n",
    "* Please submit your python code for all the tasks where it is applicable. Make sure to include comments explaining complicated/non-obvious sections of your code.\n",
    "* Please also submit a copy of the console output of your code execution. Your code might run in 10 minutes on your watercooled battlestation, but it might not run in 10 minutes for the person who corrects your home exercises. Thank you!\n",
    "    * If your code didn't finish running, please be honest and add a short statement saying so. Thank you again!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.0 Creating Data Splits (0P)\n",
    "The following code generates train, dev and test sets for the 20 Newsgroups dataset (see [hex07_data.zip](https://moodle.informatik.tu-darmstadt.de/pluginfile.php/55190/mod_assign/introattachment/0/hex07_data.zip?forcedownload=1) in Moodle). You might need to modify the `input_file` variable according to your setup.\n",
    "After execution `train_y` is a list of 20-component one-hot vectors representing newsgroups, and `train_x` is a list of 300-component vectors where each entry corresponds to a word ID. Each 300-component vector represents an e-mail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import random\n",
    "import os\n",
    "\n",
    "input_file = 'data.txt'\n",
    "tmp_dir = '/tmp'\n",
    "train_verbose = 1\n",
    "pad_length = 300\n",
    "\n",
    "def read_data(input_file):\n",
    "    vocab = {0}\n",
    "    data_x = []\n",
    "    data_y = []\n",
    "    with open(input_file) as f:\n",
    "        for line in f:\n",
    "            label, content = line.split('\\t')\n",
    "            content = [int(v) for v in content.split()]\n",
    "            vocab.update(content)\n",
    "            data_x.append(content)\n",
    "            label = tuple(int(v) for v in label.split())\n",
    "            data_y.append(label)\n",
    "\n",
    "    data_x = pad_sequences(data_x, maxlen=pad_length)\n",
    "    return list(zip(data_y , data_x)), vocab\n",
    "\n",
    "data, vocab = read_data(input_file)\n",
    "vocab_size = max(vocab)\n",
    "random.seed(42)\n",
    "random.shuffle(data)\n",
    "input_len = len(data)\n",
    "train_y, train_x = zip(*(data[:(input_len * 8) // 10]))\n",
    "dev_y, dev_x = zip(*(data[(input_len * 8) // 10: (input_len * 9) // 10]))\n",
    "test_y, test_x = zip(*(data[(input_len * 9) // 10:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1 - Dataset inspection (1 P)\n",
    "Complete the given function for a basic inspection of the given dataset. The function should return the majority baseline accuracy (the expected accuracy when a classifier always chooses the most likely class) that is expected for the given dataset, as well as its number of classes as a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inspect_dataset(labels):\n",
    "    return 0, 0\n",
    "\n",
    "baseline, num_classes = inspect_dataset(train_y)\n",
    "print('Majority baseline accuracy is %f over %i classes' % (baseline, num_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 - A basic CNN (2 P)\n",
    "Build a basic CNN by completing the code below that contains\n",
    "\n",
    "* A convolutional layer with 75  filters and kernel size 3, using a ReLU activation function\n",
    "* A (global) max pooling layer\n",
    "* A softmax output layer\n",
    "\n",
    "How do you rate your model's performance with respect to the established baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import numpy as np\n",
    "train_x, train_y = np.array(train_x), np.array(train_y)\n",
    "dev_x, dev_y = np.array(dev_x), np.array(dev_y)\n",
    "test_x, test_y = np.array(test_x), np.array(test_y)\n",
    "\n",
    "# Leave those unmodified and, if requested by the task, modify them locally in the specific task\n",
    "batch_size = 64\n",
    "embedding_dims = 100\n",
    "epochs = 2\n",
    "filters = 75\n",
    "kernel_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dims, input_length=pad_length))\n",
    "\n",
    "#####\n",
    "# Your implementation here\n",
    "\n",
    "#####\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_x, train_y, batch_size=batch_size, epochs=epochs, verbose=train_verbose)\n",
    "print('Accuracy of simple CNN: %f' % model.evaluate(dev_x, dev_y, verbose=0)[1])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3 - Early Stopping (2 P)\n",
    "Based on the basic CNN from task 2, create a new model that uses early stopping to determine the optimal number of epochs. This can be done by setting a high number of epochs (say 50) initially, recording the best achieved result on the dev set during training, and early stopping if there are no improvements after a set amount of epochs (say 4).\n",
    "\n",
    "To implement this in Keras, callbacks can be supplied to the training process. In particular, have a look at [Model Checkpoint](https://keras.io/callbacks/#modelcheckpoint) and [EarlyStopping](https://keras.io/callbacks/#earlystopping).\n",
    "\n",
    "Determine the model as described above and reports its results on dev and test set. Don't forget to load the best weights after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.4 - Testing CNN hyperparameters (3.5 P)\n",
    "Based on the basic CNN from task 2, create additional models to test the effect of the following hyperparameters/modifications:\n",
    "\n",
    "1. Number of filters\n",
    "1. Kernel size\n",
    "1. Dropout\n",
    "1. Stride size\n",
    "1. Pooling strategies\n",
    "1. Number of Convolutional Layers, with different kernel sizes in each (1P for this hyperparameter, 0.5P for every other)\n",
    "\n",
    "For each of these, report 3 tested configurations on the dev set. At the end, provide a short summary of the gained insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.5 - Freestyle (1.5 P)\n",
    "The task is simple: Construct a model of your choice to achieve the best performace. You may modify every parameter to your liking (except `pad_length` from the given code).\n",
    "\n",
    "Report your 3 best identified configurations. Hand in your best found configuration in code and report its performance on the test set.\n",
    "\n",
    "*Can you beat an accuracy of 85%?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
