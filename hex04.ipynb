{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Q: What is the main difference between CBOW and Skip-Gram in comparison to previous models?\n",
    "    Compared to feed forward and recurrent neural networks the new models are much simpler.\n",
    "\n",
    "\n",
    "### Q: What is the benefit of this difference?\n",
    "\n",
    "    Because of the much lower computational complexity, it is possible to compute very accurate high dimensional word vectors from a much larger data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from random import randint\n",
    "from nltk.corpus import brown\n",
    "from random import shuffle\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a python method which reads the dataset into an appropriate format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filereader(textfile):\n",
    "    #read file from disk into memory\n",
    "    f = open(textfile)\n",
    "    raw = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    entries = raw.split('\\n')\n",
    "    data = []\n",
    "    for i in range(len(entries)):\n",
    "        if(len(entries[i].split('\\t')) > 2 and i != 0):\n",
    "            entry = entries[i].split('\\t')\n",
    "            entry = tuple([entry[0], entry[1], float(entry[2])])\n",
    "            data.append(entry)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the pretrained binary word2vec embeddings with gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#load ws-353 data and word vectors\n",
    "ws353 = filereader('ws-353/combined.tab')\n",
    "word_vectors = KeyedVectors.load_word2vec_format('/home/maximilian/Downloads/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the pairwise word similarity between each word pair using gensim's similarity method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "human_scores = []\n",
    "ai_scores = []\n",
    "\n",
    "#iterate through data entries and compute gensim similarity\n",
    "for entry in ws353:\n",
    "    word1 = entry[0]\n",
    "    word2 = entry[1]\n",
    "    sim_score_human = entry[2]/10\n",
    "    sim_score_ai = float(word_vectors.similarity(word1, word2))\n",
    "    \n",
    "    #store scores in comparable lists\n",
    "    human_scores.append(sim_score_human)\n",
    "    ai_scores.append(sim_score_ai)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the coefficient between the values assigned by humans and your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spearmanr(human_scores, ai_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain in two sentences what your resulting coefficient means.\n",
    "    The resulting coefficient 0.7 means that gensim's similarity \n",
    "    method tends to rank word similarities like humans do. \n",
    "    That means that humans and the similarity method will agree \n",
    "    in 70 percent of cases that one given word pair is more similar than another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.2 Gamified Word Intrusion (4P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load corpus to draw words from\n",
    "tagged_words = brown.tagged_words(tagset='universal')\n",
    "length = len(tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define constraints for a suitable 'parent word'\n",
    "def pw_constraints(tpw):\n",
    "    word = tpw[0]\n",
    "    tag = tpw[1]\n",
    "    if(tag not in['NOUN','ADJ','ADV', 'VERB']):\n",
    "        return False\n",
    "    if(word in word_vectors.vocab.keys()):\n",
    "        if(word_vectors.most_similar(word)[2][1] > 0.65):\n",
    "            for simword in word_vectors.most_similar(word):\n",
    "                if(simword[0] in ['is', 'be', 'have', 'can', 'would', 'do'] or str.lower(simword[0][:3]) == str.lower(word[:3])):\n",
    "                    return False\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#define constraints for a suitable 'outlier word'\n",
    "def ow_constrainsts(tpw, tow):\n",
    "    word1 = tpw[0]\n",
    "    word2 = tow[0]\n",
    "    tag = tow[1]\n",
    "    if(tag not in['NOUN','ADJ','ADV', 'VERB']):\n",
    "        return False\n",
    "    if(word1 in word_vectors.vocab.keys() and word2 in word_vectors.vocab.keys()):\n",
    "        if(float(word_vectors.similarity(word1, word2)) < 0.2):\n",
    "            for simword in word_vectors.most_similar(word2):\n",
    "                if(simword[0] not in ['is', 'be', 'have', 'can', 'would', 'do']):\n",
    "                    return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define constraints for user input    \n",
    "def getSelectionInput():\n",
    "    val = 0\n",
    "    while(val > 5 or val <= 0):\n",
    "        userInput = input('Enter a number:')\n",
    "        try:\n",
    "           val = int(userInput)\n",
    "        except ValueError:\n",
    "           print(\"That's not an int!\")\n",
    "           val=0\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find four words which are similar to each other according to the pretrained word2vec embeddings, and one intruder word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate lists of similar words and 1 unsimilar word. store them in quintuple\n",
    "quintuples = []\n",
    "n_word_tuples = 5 #number of questions presented to the player\n",
    "\n",
    "for i in range(n_word_tuples):\n",
    "    \n",
    "    #retreive a suitable random parent word\n",
    "    tpw = tagged_words[randint(0,length-1)] #random tagged_parent_word drawn from brown corpus\n",
    "    while(pw_constraints(tpw) is not True):     #make sure it's suitable according to my constraints\n",
    "        tpw = tagged_words[randint(0,length-1)]\n",
    "        \n",
    "    \n",
    "    #retreive a suitable outlier word\n",
    "    tow = tagged_words[randint(0,length-1)] #random tagged outlier word drawn from brown corpus\n",
    "    while(ow_constrainsts(tpw, tow) is not True): #make sure it's suitable according to my constraints\n",
    "        tow = tagged_words[randint(0,length-1)]\n",
    "        \n",
    "    #generate quintuple of 4 similar and an outlier word\n",
    "    q1 = word_vectors.most_similar(tpw[0])\n",
    "    q = [tpw[0], q1[0][0],q1[1][0],q1[2][0], tow[0]]\n",
    "    quintuples.append(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the five words in random order, then query a human (i.e. yourself) to spot the intruder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Spot the intruder!\")\n",
    "\n",
    "#initialize confusion matrix for accuracy computation\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "\n",
    "\n",
    "for quintuple in quintuples:\n",
    "    #put words into random order and store the outlier\n",
    "    outlier = quintuple[4]\n",
    "    shuffle(quintuple)\n",
    "    \n",
    "    #Print the five words\n",
    "    for j in range(len(quintuple)):\n",
    "        print(str(j+1)+\":  \", quintuple[j])\n",
    "    print('\\n')\n",
    "    \n",
    "    #Query user and and update confusion matrix based on correctness of user input\n",
    "    if(outlier == quintuple[getSelectionInput()-1]):   \n",
    "        TP += 1\n",
    "        TN += 4\n",
    "    else:\n",
    "        FP += 1\n",
    "        FN += 1\n",
    "        TN += 3\n",
    "    \n",
    "    \n",
    "\n",
    "#calculate and print accuracy    \n",
    "accuracy = (TP+TN)/(TP+TN+FP+FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the accuracy currently reached by the human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('accuracy:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
