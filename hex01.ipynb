{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL4NLP SS17 Home Exercise 01\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 Setup (1P)\n",
    "Install Python 3, numpy and Jupyter on your machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 Perceptron Learning\n",
    "### Task 2.1 Sigmoid Activation Function (1P)\n",
    "When optimizing functions, first or higher-order derivatives (gradients, Hessians) are of major importance. In neural network learning, we typically want to minimize weight parameters so that the difference between the net output and the true labels is minimized, e.g.:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\text{min}_\\mathbf{w}\\sum_{j=1}^N \\Bigl(\\sigma(\\mathbf{x}_j \\cdot \\mathbf{w})-y_j\\Bigr)^2\n",
    "\\end{equation}\n",
    "\n",
    "Here, $\\sigma$ is an activation function. A frequently used activation function is the *sigmoid* function, defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\text{sig}(x) = \\frac{1}{1+\\exp(-x)}\n",
    "\\end{equation}\n",
    "\n",
    "Show that:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{sig}'(x) = \\text{sig}(x) \\cdot \\bigl(1-\\text{sig}(x)\\bigr)\n",
    "\\end{equation}\n",
    "\n",
    "You may find the chain rule useful: $f(g(x))' = f'(g(x))\\cdot g'(x)$\n",
    "\n",
    "### Task 2.2 Perceptron Learning by Hand (2P)\n",
    "A simple perceptron learning algorithm was introduced in the lecture (slide 88). Here is the weight update rule again for reference:\n",
    "\\begin{equation}\n",
    "    w' \\leftarrow w - \\alpha \\sum_{(\\mathbf{x},y)\\in\\mathcal{T}'} \\Bigl(\\sigma(\\mathbf{x} \\cdot \\mathbf{w}) - y\\Bigr) \\cdot \\sigma'(\\mathbf{x} \\cdot \\mathbf{w}) \\cdot x^T\n",
    "\\end{equation}\n",
    "The weight update rule is designed to minimize the square loss between the perceptron output and the target labels (see slide 87).\n",
    "\n",
    "#### a) Training\n",
    "\n",
    "Train a perceptron using the abovementioned algorithm and report the weight vector $w_j$ after each weight update. Run one epoch (one training pass over the training data) with the following parameters:\n",
    "* activation function $\\sigma = \\text{sig}$ (see Task 2)\n",
    "* initial weight vector $w_0 = (-1, 1)^T$\n",
    "* learning rate $\\alpha = 1$\n",
    "* batch size $N'=1$, i.e. one weight vector update per data point $(\\mathbf{x}, y)$\n",
    "* training data $T$:\n",
    "\n",
    "| $j$ | $x_1$ | $x_2$ | $y$ |\n",
    "|----:|------:|------:|----:|\n",
    "|  1  | -1.28 |  0.09 |  0  |\n",
    "|  2  | 0.17  |  0.39 |  1  |\n",
    "|  3  | 1.36  |  0.46 |  1  |\n",
    "|  4  | -0.51 | -0.32 |  0  |\n",
    "\n",
    "#### b) Evaluation\n",
    "\n",
    "Compute the square loss $L$ before (using $w_0$) and after training (using $w_4$) on the following test data:\n",
    "\n",
    "| $j$ | $x_1$ | $x_2$ | $y$ |\n",
    "|----:|------:|------:|----:|\n",
    "|  1  | -0.50 | -1.00 |  0  |\n",
    "|  2  |  0.75 |  0.25 |  1  |\n",
    "\n",
    "Square loss is defined as (see Task 2 and slide 87):\n",
    "\\begin{equation}\n",
    "    L = \\sum_{j=1}^N \\ell(\\mathbf{x}_j, y) = \\sum_{j=1}^N (\\sigma(\\mathbf{x}_j \\cdot \\mathbf{w}) - y_j)^2\n",
    "\\end{equation}\n",
    "\n",
    "### Task 2.3 Decision Boundary and Plotting (1P)\n",
    "A perceptron learns a linear decision boundary. The activation function $\\sigma(\\mathbf{x} \\cdot \\mathbf{w})$ used throughout this exercise corresponds to a decision boundary $x_1 \\cdot w_1 + x_2 \\cdot w_2 = 0$ (Hesse normal form).\n",
    "\n",
    "Create a plot with [matplotlib](https://matplotlib.org/contents.html) that shows:\n",
    "* the training and test data points from Task 2.2\n",
    "* the decision boundaries before and after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.28  0.09]]\n",
      "0.0\n",
      "0.797380153569\n",
      "[[-0.83509919]\n",
      " [ 0.98840541]]\n",
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#create training data\n",
    "training_data_input_list = [[-1.28, 0.09, 0],\n",
    "                            [ 0.17, 0.39, 1],\n",
    "                            [ 1.36, 0.46, 1],\n",
    "                            [-0.51,-0.32, 0]]\n",
    "training_data = np.matrix(training_data_input_list)\n",
    "\n",
    "w0 = np.matrix([[-1],[1]])\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-1*x))\n",
    "\n",
    "def w_update(weight_vector, data_point_shape_n_1, activation_function):\n",
    "    x_vector = data_point_shape_n_1[0,:-1]\n",
    "    print(x_vector)\n",
    "    y_scalar = data_point_shape_n_1[0,-1]\n",
    "    w = weight_vector\n",
    "    print(y_scalar)\n",
    "    \n",
    "    activation = activation_function(np.dot(x_vector,w).astype('float64'))[0,0]\n",
    "    print(activation)\n",
    "    \n",
    "    erg = w - (activation - y_scalar) * (activation*(1-activation))* x_vector.transpose()\n",
    "    print(erg)\n",
    "    print(x_vector.shape)\n",
    "    return 0\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    #choose one random sample with from training data\n",
    "    w_update(w0, training_data[0,:], sigmoid)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
