{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL4NLP SS17 Home Exercise 06\n",
    "----------------------------------\n",
    "**Due until Tuesday, 06.06. at 13:00**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 Mandatory Paper (1P)\n",
    "\n",
    "Read [this week's mandatory paper](http://www.aclweb.org/anthology/P16-1089) on sentence representations and explain the training objective in two to three sentences. Which \"trick\" does it remind you of?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 Training word2vec Word Embeddings (1P)\n",
    "In this task, you will train word2vec Skip-Gram Embeddings yourself.\n",
    "\n",
    "#### Data\n",
    "The embeddings will be trained on 100K sentences taken from the English Wikipedia. You can find the sentences in the ``en_wiki_dump_100k.txt`` file within [hex06_data.zip](https://moodle.informatik.tu-darmstadt.de/pluginfile.php/55129/mod_assign/introattachment/0/hex06_data.zip?forcedownload=1) on Moodle.\n",
    "\n",
    "#### Word2Vec\n",
    "Download (or `git clone`) word2vec from here: https://github.com/shenoybr/word2vec and compile the tool. On Linux, this can be achieved by calling \"``make``\" in the word2vec directory. If you are a Windows user; [here are several alternatives to make](https://stackoverflow.com/questions/2532234/how-to-run-a-makefile-in-windows). Another possibility is to visit the ISP pool in the Piloty building (or to use ``ssh`` from home).\n",
    "\n",
    "### Task 2.1 word2vec Parameters (0.5P)\n",
    "Run the program without specifying any arguments (for example by ``./word2vec`` if on Linux). This will show you an example of how to run word2vec with given parameters:\n",
    "\n",
    "``./word2vec -train data.txt -output vec.txt -size 200 -window 5 -sample 1e-4 -negative 5 -hs 0 -binary 0 -cbow 1 -iter 3``\n",
    "\n",
    "What are the effects of the four parameters ``size``, ``window``, ``negative``, and ``cbow``?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 Training (0.5P)\n",
    "Now train word2vec with the following hyperparameters, leaving all others at their default: \n",
    "* size=300\n",
    "* window=5\n",
    "* cbow=0\n",
    "* iter=10\n",
    "\n",
    "Then, use the ``distance`` tool in the Word2Vec directory and compute:\n",
    "* the 5 closest words to the word \"man\"\n",
    "* the 5 closest words to the word \"woman\"\n",
    "\n",
    "What do you notice in the output?\n",
    "\n",
    "**Note**: ``distance`` requires a model file in binary format. Therefore, set the binary flag to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 Training Sent2Vec Embeddings (1P)\n",
    "In this task, you will train [Sent2Vec Embeddings](https://arxiv.org/abs/1703.02507) yourself. Sent2Vec is an embedding model that, similar to Siamese-CBOW, optimizes word embeddings for the purpose of averaging them: the average embedding of a list of words should yield a good sentence representation of the sentence containing the words. However, the model underlying Sent2Vec is different from Siamese-CBOW, and allows for faster processing.\n",
    "\n",
    "Download (or ``git clone``) Sent2Vec from here: https://github.com/epfml/sent2vec and compile the tool.\n",
    "\n",
    "### Task 3.1 Parameters (0.5P)\n",
    "Run the tool without parameters (``./fasttext sent2vec`` on Linux) and take a look at the arguments of the tool.\n",
    "\n",
    "What are the effects of the three parameters ``lr``, ``dim``, and ``ws``?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2 Training (0.5P)\n",
    "Now train embeddings on the Wikipedia dump from Task 2 using the following hyperparameters, leaving all others at their defaults:\n",
    "* dim=300\n",
    "* wordNgrams=2\n",
    "  \n",
    "Which command is necessary to convert a sentence into a vector representation, given a previously trained model? Assume that your sentences are in a file \"mysentences.dat\" in which each line consists of a tokenized sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 Sentence Embeddings for Movie Reviews (2P)\n",
    "We return once again to the movie reviews dataset labeled with sentiment polarity. This time, the task is to compare two sentence representation techniques: sent2vec sentence embeddings and word2vec embeddings averaged on the word level.\n",
    "\n",
    "#### Data\n",
    "[hex06_data.zip](https://moodle.informatik.tu-darmstadt.de/pluginfile.php/55129/mod_assign/introattachment/0/hex06_data.zip?forcedownload=1) contains the training, development and test reviews and their labels in separate text files.\n",
    "\n",
    "#### Hints on the Submission Format\n",
    "* Please submit your python code for all the tasks where it is applicable. Make sure to include comments explaining complicated/non-obvious sections of your code.\n",
    "* Please also submit a copy of the console output of your code execution. Your code might run in 10 minutes on your watercooled battlestation, but it might not run in 10 minutes for the person who corrects your home exercises. Thank you!\n",
    "    * If your code didn't finish running, please be honest and add a short statement saying so. Thank you again!\n",
    "\n",
    "### Task 4.1 Creating Embeddings For Reviews (1P)\n",
    "\n",
    "a) Obtain a Sent2Vec embedding for each movie review. To do so, apply the command from task 3.2 with the model trained in task 3.2 on each of the ``rt-polarity.*.reviews.txt`` files and write the output to a file.\n",
    "\n",
    "b) Obtain a word2vec embedding for each movie review. To do so, write a python script, which:\n",
    "* reads the review files \n",
    "* loads the word2vec embeddings trained in task 2.2\n",
    "* computes an embedding for a full review by averaging the word embedding of each word in the review\n",
    "\n",
    "Hint: Find a suitable way to deal with out-of-vocabulary words.\n",
    "\n",
    "**Print the sent2vec and the averaged word2vec embedding vector of the first review from the training data set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2 Comparison (1P)\n",
    "Implement a perceptron which takes an embedding of a review as an input, applies softmax and uses cross-entropy as the loss function. You may adapt your implementation/the solution from home exercise 02 or write a new one with TensorFlow or Keras.\n",
    "Remember that cross-entropy loss requires class labels in a one-hot vector format.\n",
    "\n",
    "Train the perceptron with sent2vec embeddings on the training set (hyperparameters are up to you) and report its accuracy on the test set. Repeat the procedure with your averaged word2vec embeddings. Give a one-sentence comment on the results. \n",
    "\n",
    "**0.5P Bonus:** State a likely reason why the test set accuracy is higher/lower than the 71% accuracy achieved in home exercise 02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
